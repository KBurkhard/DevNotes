
Lesson 02: Naive Bayes

- classification from supervised learning
  - take features and turn them into labels

- scatter plots
  - take data and transform into a decision surface
  - decision surface will seperate 2 classes of points
    - can be linear

- Python code
  - sklearn library
    - ml libarys
    - use sklearn Gaussian naive bayes
    
- fitting
  - always train and tets on seperate sets of data
    - if data overlaps, you can overfit to your data
    - so always save 10% of your data to use as your testing set
      - use the test result to report progress of your ML
      
- bayes rule
  - rev thomas bayes
  - cancer exampe
    - P(c) = 0.01 probability of cancer
    - test:
      - 90% correct if positive and you have cancer
        - called sensitivity
      - 90% correct if negative and do not have cancer
        - called specitivity
    - question: if your test is positive, what is the probability that you have cancer
  - explained
    - prior: prob before you run a test
    - then get test evidence when you run the test
    - result in posterior prob
    - so bayes incorporates some information from a test into your prior to arrive to your posterior
    - in cancer
      - prior is cancer chance
      - posterior is prob of cancer given positive test
      - so values are:
        - P(c) = 0.01 - probability of having cancer
        - P(!c) = 0.99 - prob of no cancer
        - P(positive test | c) = 0.9 (prob of positive result given have cancer - sentivity)
        - P(!positive | !c) = 0.9 (prob of negative result if no cancer - specitivity)
        - P(pos | !c) = 1 - P(!pos | !c) = 0.1
        
      - question - if you pos, what chances you have cancer
      - P(have cancer | positive test) = P(c) x P(positive | have cancer)
        - P(c | pos) = 0.01 x 0.9 = 0.009
      - P(no cancer | positive test) = P(!c) x (positive | no cancer)
        - P(!c | pos) = P(!c) x P(pos | !c) = 0.99 x 0.1 = 0.099
    - normalize
      - need both probs to add up to 1; grow them keeping ratio consistent
      - P(c | pos) + P(!c | pos) = 0.009 + 0.099 = 0.108
      - P(pos) = P(c | pos) + P(!c | pos) = 0.108 -> should be 1, so normalize
        - P(c | pos) = 0.009 / 0.108 = 0.0833
        - P(!c | pos) = 0.099 / 0.108 = 0.9167
      
    
    
    
      
    